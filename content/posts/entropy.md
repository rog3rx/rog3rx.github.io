---
title: "Entropy"
date: 2025-03-09T22:00:47+08:00
draft: false
math: true
---

## 1. 我们如何度量信息？

### 什么是信息？

要度量信息之前，我们首先要定义什么是信息。一个最简单、但稍微抽象的理解是：**信息就是知识**。

从直观的感受上来说，当你观察到一件事情后所收获的知识，正反映了该事件所包含的信息量。一般情况下，对于不同发生概率的事件我们会有如下的感受：

- **当高概率事件发生时**：你早就预料到了结果，因此知识增量很少，信息量也很低。
- **当低概率事件发生时**：你可能会获得更多新知识，信息量也更高。

### 惊讶度与概率的关系

我们可以将上述里面获取的知识类比为“惊喜程度”。假设我们抛掷一枚“硬币”，假设抛掷硬币的随机变量 $X$， H 表示正面，T 表示反面:

- $Pr(X=H) = p$
- $Pr(X=T) = 1 - p$
- 当 $p = 1$ 或 $p = 0$ 时，硬币结果完全可预测，毫无惊喜可言。
- 当 $p = 0.5$ 时，硬币是公平的，结果最为不确定，无论出现正面或反面，你都会有一定程度的惊讶。

因此我们希望对于一个发生概率为 p 的事件，惊讶度 $S(p)$ 满足：

- 结果确定（$p = 1$）时，惊讶度为0。
- 结果概率越小，惊讶度越高。
- 连续独立事件带来的惊讶度应该具有可加性。

最终可以得到一个具有该性质的函数，即惊讶度函数 $S(p)$：

$S(p) = \log\left(\frac{1}{p}\right) = -\log(p)$

### 平均惊讶度（熵）

单次抛掷硬币的结果是随机的，我们可能看到 H 或 T。因此，平均（期望）惊讶，即我们平均预期感受到的惊讶程度为：

$\text{Expected Surprise} = -[p \log(p) + (1 - p) \log(1 - p)]$

上述公式就是熵（Entropy）的定义。

---

### 熵与混乱（无序）的关系

熵这个概念最初来自热力学，常被描述为衡量无序、不确定性或信息的标准。

在物理学中，熵与热力学第二定律紧密相关：

> 孤立系统的熵总是随时间增加。自然过程促使系统从有序状态转变为无序状态。

例如，一杯热咖啡放在桌子上：

- 起初咖啡很热，空气较凉，系统处于低熵状态。
- 随着热量扩散到周围空气中，咖啡逐渐冷却，热量分布均匀，系统熵增加，这个过程是不可逆的。

再比如，一副牌的状态变化：

- 排列整齐的牌是低熵状态（高度有序）。
- 洗牌后的牌杂乱无章，熵变高。
- 打乱牌组比重新整理更容易，这也体现了熵增加的自然趋势。

假设一本书可能出现的位置包括：书架、床上、沙发下、书桌上。

- 若书总是在书架上（$p = 1$），系统完全可预测，熵为 0，表示完全有序。
- 若书均匀随机地可能出现在四个位置中，概率分布相同（$p = 0.25$），系统的熵达到最大，表示完全无序。

**熵与有序性的直观理解：**

- **高熵**：所有情况的结果可能性相等，无法预测具体结果，即该系统是混乱、无序的。
- **低熵**：某些结果发生概率很高，那么极有可能被预测，系统更偏向有序。

对上面直观到抽象的概念进行总结，我们可以得到熵的完整定义：

### 熵（Entropy）：系统内在的不确定性

熵是衡量随机变量不确定性的指标。对于二元分布（例如抛硬币），熵的公式为

$H(p) = -p \log p - (1-p) \log (1-p)$

**内在属性：** 需要注意的是，这里的熵是系统内在的属性，与观察者的主观认识无关。即使我们不知道硬币是否公平，其熵仍由真实概率 $p$ 决定。

---

## 2. 交叉熵（Cross-Entropy）：结合真实概率与主观信念的“惊讶”

- **真实与估计：**
  在实际问题中，我们往往不知道硬币的真实概率 $p$。假设我们对正面概率的估计为 $q$，反面则为 $1-q$。

- **惊讶值的计算：**
  在计算期望时，每种情况的期望惊讶值等于该结果的发生概率乘以“惊讶”值，其中“惊讶”被我们认为的概率量化为：

  - 正面：$-\log(q)$
  - 反面：$-\log(1-q)$

  而实际出现的概率分别为 $p$ 和 $1-p$。

最终，用实际发生的频率对惊讶值进行加权后，得到交叉熵的公式：

  $\text{Cross-Entropy}(P, Q) = -\bigl[ p \log(q) + (1-p) \log(1-q) \bigr]$

**直观理解：**：如果你认为硬币是公平的（$q = 0.5$），但实际上硬币有 90% 的概率正面朝上（$p = 0.9$），你面对真实结果时就会比预期更加“惊讶”。随着你不断更新信念 $q$ 以使之更接近 $p$，平均惊讶程度（即交叉熵）会逐渐降低。当 $q$ 与 $p$ 完全一致时，交叉熵达到最小值，等于系统固有的不确定性（熵）。

**不对称性：**：交叉熵是不对称的，因为真实分布 $P$ 决定了结果发生的频率，而我们估计的 $Q$ 则塑造了我们对结果的惊讶感。

---

## 3. Kullback–Leibler 散度 (KL Divergence)：多余惊讶的量化

- **定义与公式：**
  KL 散度用于衡量交叉熵与真实熵之间的差值，即由错误的预测信念带来的额外“惊讶”：

  $\text{KL}(P \parallel Q) = \text{Cross-Entropy}(P, Q) - H(P)$

- **当 $q = p$ 时：**
  如果你的估计 $q$ 完美匹配真实概率 $p$，则 KL 散度为零，表示没有额外的惊讶，预测与实际完全一致。

- **交叉熵的分解：**
  可以理解为交叉熵包含了系统固有的不确定性（熵）以及因预测不完美而引入的额外惊讶：

  $H(P, Q) = H(P) + \text{something}$

  而那个“某些东西”就是 KL 散度：

  $\text{KL}(P \parallel Q) = H(P, Q) - H(P)$

- **吉布斯不等式：**
  根据吉布斯不等式，总是有

  $\text{KL}(P \parallel Q) \geq 0$

  因此必然有

  $H(P, Q) \geq H(P)$

  这说明使用与真实分布不匹配的模型总会引入额外的信息损失或“惊讶”。

总结一下熵、交叉熵和 KL 散度的联系与区别：

- **熵** 作为系统固有的不确定性；
- **交叉熵** 则是结合真实概率和我们主观信念下的期望“惊讶”，等于使用模型 $Q$ 去描述真实分布 $P$ 时所产生的平均惊奇度。
- **KL 散度** 则量化了由于模型预测与真实分布不匹配而带来的额外惊讶或信息损失，等于交叉熵减去真实分布的熵，反映了模型与真实分布之间的“额外”惊奇度或误差。

通过不断调整我们的预测 $q$ 以接近真实分布 $p$，可以减少交叉熵和 KL 散度，从而提高模型的准确性。

在机器学习中，我们通常以最小化交叉熵（等价于最小化 KL 散度）作为训练目标，因为真实分布的熵为常数，不影响模型优化的方向。

--- 

## 似然比角度推演 KL 散度

下面我们再从另一个视角解释和推演 KL 散度和交叉熵

在深度学习中，我们的模型常常是用来模拟和预测真实世界中的概率分布。因此我们常常面对一个“真实”分布 $P$（即真实的数据生成过程）和一个“模型”分布 $Q$（模型关于数据的猜测）。所以我们希望有一种方法来衡量 $P$ 与 $Q$ 之间的差异程度。

还是以最简单的硬币进行一个简单的讲解，假设有一枚公平的硬币，正面和反面的概率均为 0.5
另一枚硬币，正面概率是 0.9，反面是 0.1

我们如何估计这两个概率分布的距离？
但是我们有个直观的感受，如果第二枚硬币的正面概率为 0.55，那么第二枚硬币明显更接近公平的硬币，也就是说概率分布更加的近似。但如果是正面概率为0.9的硬币，我们也可以很快感受到和公平硬币的差距。

那么如何度量这种距离呢？一个简单的想法，就是可以直接计算在假设的概率下，事件发生的可能性。如果发生的概率相似，那么很有可能是相似的。这个值其实有个专门的名字，就叫“似然值”。

“似然值”通常指在已知一个统计模型和其参数的情况下，观察到当前数据的概率。换句话说，假设我们有一个模型，其行为由某些参数控制，那么当这些参数取特定值时，数据出现的可能性就被称为这个参数下的似然值。

还是拿投掷硬币举例，观察到的结果是正面和反面各出现了若干次。我们可以用一个参数 $p$ 表示正面朝上的概率。似然函数就可以写成：

$L(p) = p^{\text{正面次数}} (1-p)^{\text{反面次数}}$


现在我们有两个硬币，一个“公平”硬币，一个“不公平”硬币。

*   **硬币 1（真实）**：正面概率 $p$，反面概率 $1-p$。

*   **硬币 2（模型）**：正面概率 $q$，反面概率 $1-q$。

我们掷“真实”硬币 $n$ 次，记录正面次数 $n_H$ 和反面次数 $n_T$。则：

1.  **$P$ 下的似然度**：

    $$

    \mathcal{L}_P = p^{n_H}(1 - p)^{n_T}.

    $$

2.  **$Q$ 下的似然度**：

    $$

    \mathcal{L}_Q = q^{n_H}(1 - q)^{n_T}.

    $$

要比较它们，可以看似然度的比值（likelihood ratio）：

$$

\frac{\mathcal{L}_P}{\mathcal{L}_Q} = \frac{p^{n_H}(1 - p)^{n_T}}{q^{n_H}(1 - q)^{n_T}} = \left(\frac{p}{q}\right)^{n_H} \left(\frac{1 - p}{1 - q}\right)^{n_T}.

$$

### 2.3 归一化并取对数

我们希望分析一下这个比值，进行一个对数化简：

1.  **取 $n$ 次方根**，等价于把这个比值整体升到 $1/n$ 次方。

2.  **取对数**，把乘积变为求和。

因此，

$$

\ln \biggl(\bigl(\tfrac{\mathcal{L}_P}{\mathcal{L}_Q}\bigr)^{\frac{1}{n}}\biggr) = \frac{1}{n} \ln \bigl(\mathcal{L}_P\bigr) - \frac{1}{n} \ln \bigl(\mathcal{L}_Q\bigr).

$$

带入 $\mathcal{L}_P$ 和 $\mathcal{L}_Q$：

$$

\frac{1}{n} \ln \bigl(\mathcal{L}_P\bigr) = \frac{1}{n} \left( n_H \ln p + n_T \ln(1-p) \right) = \frac{n_H}{n} \ln p + \frac{n_T}{n} \ln (1-p),

$$

对于 $Q$ 的情况做同样的处理可得：

$$

\frac{1}{n} \ln \bigl(\tfrac{\mathcal{L}_P}{\mathcal{L}_Q}\bigr) = \biggl(\frac{n_H}{n}\biggr)\ln\Bigl(\frac{p}{q}\Bigr) + \biggl(\frac{n_T}{n}\biggr)\ln\Bigl(\frac{1 - p}{1 - q}\Bigr).

$$

当 $n \to \infty$ 时，依照大数定律：

$$

\frac{n_H}{n} \to p,\quad

\frac{n_T}{n} \to 1-p.

$$

因为我们假设 p 是真实世界的概率，因此，

$$

\lim_{n\to\infty} \frac{1}{n} \ln \bigl(\tfrac{\mathcal{L}_P}{\mathcal{L}_Q}\bigr) = p \ln\Bigl(\frac{p}{q}\Bigr) + (1-p)\ln\Bigl(\frac{1-p}{1-q}\Bigr).

$$

这正是两个 Bernoulli 分布间 KL 散度的形式。

---

### KL 散度与机器学习中的交叉熵

在监督学习特别是分类任务中，我们常常这样描述：

- **模型对类别 $i$ 的预测**为 $Q(i) = \hat{y}_i$。
- **真实分布**常常是一种“独热”（one-hot）向量，即正确类别概率为 1，其余类别概率为 0。

我们最常用的损失函数是 **交叉熵（Cross-Entropy）**：

$$
H(P,Q) = -\sum_{i} P(i)\,\ln Q(i).
$$

由于 $P$ 是独热分布，该式化简为 $-\ln Q(\text{正确类别})$。与此同时，

$$
D_{\mathrm{KL}}(P \| Q) = \sum_{i} P(i)\,\ln\Bigl(\tfrac{P(i)}{Q(i)}\Bigr) = \sum_{i} P(i)\,\ln\Bigl(\tfrac{1}{Q(i)}\Bigr)
\quad (\text{因为对错误类别 }P(i)=0),
$$

也就等于 $-\ln Q(\text{正确类别})$。于是就有

$$
\underbrace{H(P,Q)}_{\text{交叉熵}} = \underbrace{H(P)}_{=0\ \text{(对独热分布)}} + \underbrace{D_{\mathrm{KL}}(P \| Q)}_{\text{KL 项}} = 0 + D_{\mathrm{KL}}(P \| Q).
$$

所以，**最小化交叉熵**等价于**最小化 KL 散度**。