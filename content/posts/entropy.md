---
title: "Entropy"
date: 2025-03-09T22:00:47+08:00
draft: true
---

## 我们如何度量信息？

### 什么是信息？

首先，我们定义什么是信息。一个最简单、最抽象的理解是：**信息就是知识**。

从直观感受上来说，当你观察到一件事情后所收获的知识，正反映了该事件所包含的信息量。

- **高概率事件发生时**：你基本早就预料到结果，因此知识增量很少，信息量也很低。
- **低概率事件发生时**：你可能会获得很多新知识，信息量较高。

### 惊讶度与概率的关系

我们可以将获取的知识类比为"惊喜程度"。假设我们抛掷一枚"硬币"：

- 若硬币正面（H）出现的概率为100%，反面（T）永远不出现，那么结果毫无惊喜，因为完全可以预测。
- 若硬币正面出现的概率为$p$，则反面出现的概率为 $1-p$。

随机变量 $X$：

- $\text{Pr}(X = H) = p$
    
- $\text{Pr}(X = T) = 1 - p$
    
- 当 $p = 1$ 或 $p = 0$ 时，结果是确定的，惊讶度为零。
    
- 当 $p = 0.5$ 时，硬币是公平的，结果不确定，无论出现正面或反面，你都会感到一定程度的惊讶。
    

我们希望惊讶度 $S(p)$ 满足：

- 结果确定（$p = 1$）时，惊讶度为0。
- 结果概率越小，惊讶度越高。
- 连续独立事件带来的惊讶度应该是可加的。

因此定义惊讶度为：

$$S(p) = \log\left(\frac{1}{p}\right)$$

### 平均惊讶度（熵）

单次抛掷硬币的结果是随机的，我们可能看到 H 或 T。因此，平均（期望）惊讶，即我们平均预期感受到的惊讶程度为：

$$\text{Expected Surprise} = -[p \log(p) + (1 - p) \log(1 - p)]$$

上述公式就是熵（Entropy）的定义。

---

## 熵与混乱（无序）的关系

熵这个概念最初来自热力学，常被描述为衡量无序、不确定性或信息的标准。

在物理学中，熵与热力学第二定律紧密相关：

> 孤立系统的熵总是随时间增加。自然过程促使系统从有序状态转变为无序状态。

例如，一杯热咖啡放在桌子上：

- 起初咖啡很热，空气较凉，系统处于低熵状态。
- 随着热量扩散到周围空气中，咖啡逐渐冷却，热量分布均匀，系统熵增加，这个过程是不可逆的。

再比如，一副牌的状态变化：

- 排列整齐的牌是低熵状态（高度有序）。
- 洗牌后的牌杂乱无章，熵变高。
- 打乱牌组比重新整理更容易，这体现了熵增加的自然趋势。

### 一个具体的例子：书的位置

假设一本书可能出现的位置包括：书架、床上、沙发下、书桌上。

- 若书总是在书架上（$p = 1$），系统完全可预测，熵为 0，表示完全有序。
- 若书均匀随机地可能出现在四个位置中，概率分布相同（$p = 0.25$），系统的熵达到最大，表示完全无序。

### 熵与有序性的直观理解

- **高熵**：结果大致等可能，无法预测具体结果，即混乱、无序。
- **低熵**：某些结果发生概率很高，容易预测，系统更有序。

通过以上解释，我们能够清晰地理解熵与信息之间的关系，并将其用于描述和度量信息的多少及系统的混乱程度。


对上面的所有内容进行总结，我们可以得到熵的完整定义：

# 熵、交叉熵与 KL 散度

## 1. 熵（Entropy）：系统内在的不确定性

- **定义：**  
    熵是衡量随机变量不确定性的指标。对于二元分布（例如抛硬币），熵的公式为
    
    $$H(p) = -p \log p - (1-p) \log (1-p)$$
- **内在属性：**  
    需要注意的是，这里的熵是系统内在的属性，与观察者的主观认识无关。即使我们不知道硬币是否公平，其熵仍由真实概率 $p$ 决定。
    

---

## 2. 交叉熵（Cross-Entropy）：结合真实概率与主观信念的"惊讶"

- **真实与估计：**  
    在实际问题中，我们往往不知道硬币的真实概率 $p$。假设我们对正面概率的估计为 $q$，反面则为 $1-q$。
    
- **惊讶值的计算：**  
    在计算期望时，每种情况的期望惊讶值等于该结果的发生概率乘以"惊讶"值，其中"惊讶"被量化为：
    
    - 正面：$-\log(q)$
    - 反面：$-\log(1-q)$
    
    而实际出现的概率分别为 $p$ 和 $1-p$。
    
- **交叉熵公式：**  
    最终，实际发生的频率 $p$ 对惊讶值进行加权后，得到交叉熵：
    
    $$\text{Cross-Entropy}(P, Q) = -\bigl[ p \log(q) + (1-p) \log(1-q) \bigr]$$
- **直观理解：**  
    例如：如果你认为硬币是公平的（$q = 0.5$），但实际上硬币有 90% 的概率正面朝上（$p = 0.9$），你面对真实结果时就会比预期更加"惊讶"。随着你不断更新信念 $q$ 以使之更接近 $p$，平均惊讶程度（即交叉熵）会逐渐降低。当 $q$ 与 $p$ 完全一致时，交叉熵达到最小值，等于系统固有的不确定性（熵）。
    
- **不对称性：**  
    交叉熵是不对称的，因为真实分布 $P$ 决定了结果发生的频率，而我们的信念 $Q$ 则塑造了我们对结果的惊讶感。例如，在天气预测中，如果你总是预测 50% 的降雨概率，而实际情况几乎总是下雨，你就会不断感到意外。只有调整你的预测使之更符合现实，才能减少这种"惊讶"。
    

---

## 3. Kullback–Leibler 散度 (KL Divergence)：多余惊讶的量化

- **定义与公式：**  
    KL 散度用于衡量交叉熵与真实熵之间的差值，即由错误的预测信念带来的额外"惊讶"：
    
    $$\text{KL}(P \parallel Q) = \text{Cross-Entropy}(P, Q) - H(P)$$
- **当 $q = p$ 时：**  
    如果你的估计 $q$ 完美匹配真实概率 $p$，则 KL 散度为零，表示没有额外的惊讶，预测与实际完全一致。
    
- **交叉熵的分解：**  
    可以理解为交叉熵包含了系统固有的不确定性（熵）以及因预测不完美而引入的额外惊讶：
    
    $$H(P, Q) = H(P) + \text{something}$$
    
    而那个"某些东西"就是 KL 散度：
    
    $$\text{KL}(P \parallel Q) = H(P, Q) - H(P)$$
- **吉布斯不等式：**  
    根据吉布斯不等式，总是有
    
    $$\text{KL}(P \parallel Q) \geq 0$$
    
    因此必然有
    
    $$H(P, Q) \geq H(P)$$
    
    这说明使用与真实分布不匹配的模型总会引入额外的信息损失或"惊讶"。
    


综上所述，这部分内容说明了：

- **熵** 作为系统固有的不确定性；
- **交叉熵** 则是结合真实概率和我们主观信念下的期望"惊讶"；
- **KL 散度** 则量化了由于模型预测与真实分布不匹配而带来的额外惊讶或信息损失。

通过不断调整我们的预测信念 $q$ 以接近真实分布 $p$，可以减少交叉熵和 KL 散度，从而提高模型的准确性。

- **交叉熵**：度量使用模型 $Q$ 去描述真实分布 $P$ 时所产生的平均惊奇度。
- **KL 散度**：交叉熵减去真实分布的熵，反映了模型与真实分布之间的"额外"惊奇度或误差。
想象你预测一枚硬币的结果。如果你错误地认为硬币是公平的，但实际它被操纵，这时你观察到的惊奇度会比预期高。KL 散度正是衡量这种由于模型不准确而产生的额外惊奇感。

在机器学习中，我们通常以最小化交叉熵（等价于最小化 KL 散度）作为训练目标，因为真实分布的熵为常数，不影响模型优化的方向。
